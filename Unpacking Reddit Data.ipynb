{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae70ab57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 5 files of 0.92 gigabytes\n",
      "2012-01-14 04:02:34 : 1 : 0% : 0%\n",
      "2022-08-13 19:03:25 : 100,000 : 47% : 0%\n",
      "2024-03-03 01:10:33 : 200,000 : 78% : 0%\n",
      "2024-12-31 23:20:44 : 246,857 : 100% : 10%\n",
      "2016-03-06 17:13:12 : 246,858 : 0% : 10%\n",
      "2024-12-31 23:13:14 : 285,697 : 100% : 12%\n",
      "2016-02-18 15:54:05 : 285,698 : 0% : 12%\n",
      "2024-12-31 20:06:21 : 346,033 : 100% : 13%\n",
      "2008-12-06 20:38:10 : 346,034 : 0% : 13%\n",
      "2013-05-21 17:21:30 : 446,033 : 7% : 13%\n",
      "2014-06-03 18:16:13 : 546,033 : 10% : 13%\n",
      "2015-03-28 01:07:16 : 646,033 : 14% : 13%\n",
      "2015-12-16 05:44:11 : 746,033 : 17% : 13%\n",
      "2016-07-07 03:42:36 : 846,033 : 21% : 13%\n",
      "2017-03-12 21:58:40 : 946,033 : 28% : 13%\n",
      "2017-07-23 05:17:49 : 1,046,033 : 30% : 13%\n",
      "2018-01-01 04:59:36 : 1,146,033 : 33% : 13%\n",
      "2018-05-07 04:52:02 : 1,246,033 : 37% : 13%\n",
      "2018-08-07 01:12:57 : 1,346,033 : 41% : 13%\n",
      "2019-01-17 04:10:55 : 1,446,033 : 45% : 13%\n",
      "2019-05-11 03:35:39 : 1,546,033 : 48% : 13%\n",
      "2019-07-19 18:15:53 : 1,646,033 : 52% : 13%\n",
      "2020-01-14 22:46:38 : 1,746,033 : 58% : 13%\n",
      "2020-07-10 02:55:30 : 1,846,033 : 62% : 13%\n",
      "2020-12-30 05:38:03 : 1,946,033 : 67% : 13%\n",
      "2021-06-19 04:06:03 : 2,046,033 : 72% : 13%\n",
      "2022-02-07 23:52:51 : 2,146,033 : 77% : 13%\n",
      "2022-10-15 23:28:21 : 2,246,033 : 83% : 13%\n",
      "2023-06-01 20:42:45 : 2,346,033 : 90% : 13%\n",
      "2024-04-24 00:01:49 : 2,446,033 : 96% : 13%\n",
      "2024-12-31 23:59:22 : 2,517,360 : 100% : 82%\n",
      "2010-09-17 19:07:48 : 2,517,361 : 0% : 82%\n",
      "2022-02-13 05:24:01 : 2,617,360 : 23% : 82%\n",
      "2023-05-04 14:16:16 : 2,717,360 : 40% : 82%\n",
      "2024-04-02 03:39:12 : 2,817,360 : 76% : 82%\n",
      "2024-12-13 02:40:35 : 2,917,360 : 100% : 82%\n",
      "2024-12-31 23:58:15 : 2,925,979 : 100% : 100%\n",
      "Total: 2925979\n"
     ]
    }
   ],
   "source": [
    "# this is an example of iterating over all zst files in a single folder,\n",
    "# decompressing them and reading the created_utc field to make sure the files\n",
    "# are intact. It has no output other than the number of lines\n",
    "\n",
    "import zstandard\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import logging.handlers\n",
    "\n",
    "\n",
    "log = logging.getLogger(\"bot\")\n",
    "log.setLevel(logging.DEBUG)\n",
    "log.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "def read_and_decode(reader, chunk_size, max_window_size, previous_chunk=None, bytes_read=0):\n",
    "\tchunk = reader.read(chunk_size)\n",
    "\tbytes_read += chunk_size\n",
    "\tif previous_chunk is not None:\n",
    "\t\tchunk = previous_chunk + chunk\n",
    "\ttry:\n",
    "\t\treturn chunk.decode()\n",
    "\texcept UnicodeDecodeError:\n",
    "\t\tif bytes_read > max_window_size:\n",
    "\t\t\traise UnicodeError(f\"Unable to decode frame after reading {bytes_read:,} bytes\")\n",
    "\t\tlog.info(f\"Decoding error with {bytes_read:,} bytes, reading another chunk\")\n",
    "\t\treturn read_and_decode(reader, chunk_size, max_window_size, chunk, bytes_read)\n",
    "\n",
    "\n",
    "def read_lines_zst(file_name):\n",
    "\twith open(file_name, 'rb') as file_handle:\n",
    "\t\tbuffer = ''\n",
    "\t\treader = zstandard.ZstdDecompressor(max_window_size=2**31).stream_reader(file_handle)\n",
    "\t\twhile True:\n",
    "\t\t\tchunk = read_and_decode(reader, 2**27, (2**29) * 2)\n",
    "\n",
    "\t\t\tif not chunk:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tlines = (buffer + chunk).split(\"\\n\")\n",
    "\n",
    "\t\t\tfor line in lines[:-1]:\n",
    "\t\t\t\tyield line.strip(), file_handle.tell()\n",
    "\n",
    "\t\t\tbuffer = lines[-1]\n",
    "\n",
    "\t\treader.close()\n",
    "\n",
    "\n",
    "input_folder = r'C:\\Users\\ninuy\\Downloads\\reddit\\subreddits24'\n",
    "input_files = []\n",
    "total_size = 0\n",
    "for subdir, dirs, files in os.walk(input_folder):\n",
    "\tfor filename in files:\n",
    "\t\tinput_path = os.path.join(subdir, filename)\n",
    "\t\tif input_path.endswith(\".zst\"):\n",
    "\t\t\tfile_size = os.stat(input_path).st_size\n",
    "\t\t\ttotal_size += file_size\n",
    "\t\t\tinput_files.append([input_path, file_size])\n",
    "\n",
    "log.info(f\"Processing {len(input_files)} files of {(total_size / (2**30)):.2f} gigabytes\")\n",
    "\n",
    "total_lines = 0\n",
    "total_bytes_processed = 0\n",
    "for input_file in input_files:\n",
    "\tfile_lines = 0\n",
    "\tfile_bytes_processed = 0\n",
    "\tcreated = None\n",
    "\tfor line, file_bytes_processed in read_lines_zst(input_file[0]):\n",
    "\t\tobj = json.loads(line)\n",
    "\t\tcreated = datetime.utcfromtimestamp(int(obj['created_utc']))\n",
    "\t\tfile_lines += 1\n",
    "\t\tif file_lines == 1:\n",
    "\t\t\tlog.info(f\"{created.strftime('%Y-%m-%d %H:%M:%S')} : {file_lines + total_lines:,} : 0% : {(total_bytes_processed / total_size) * 100:.0f}%\")\n",
    "\t\tif file_lines % 100000 == 0:\n",
    "\t\t\tlog.info(f\"{created.strftime('%Y-%m-%d %H:%M:%S')} : {file_lines + total_lines:,} : {(file_bytes_processed / input_file[1]) * 100:.0f}% : {(total_bytes_processed / total_size) * 100:.0f}%\")\n",
    "\ttotal_lines += file_lines\n",
    "\ttotal_bytes_processed += input_file[1]\n",
    "\tlog.info(f\"{created.strftime('%Y-%m-%d %H:%M:%S')} : {total_lines:,} : 100% : {(total_bytes_processed / total_size) * 100:.0f}%\")\n",
    "\n",
    "log.info(f\"Total: {total_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a900e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zstandard as zstd\n",
    "import json\n",
    "\n",
    "file_path = \"RC_2023-12.zst\"  # Replace with your file name\n",
    "\n",
    "with open(file_path, 'rb') as compressed_file:\n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "    with dctx.stream_reader(compressed_file) as reader:\n",
    "        for line in reader:\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                # Example: print the comment body and subreddit\n",
    "                print(record.get(\"subreddit\"), record.get(\"body\"))\n",
    "            except json.JSONDecodeError:\n",
    "                continue  # Skip bad lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2acf92f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: nbacirclejerk_submissions.zst\n"
     ]
    },
    {
     "ename": "UnsupportedOperation",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m dctx \u001b[38;5;241m=\u001b[39m zstd\u001b[38;5;241m.\u001b[39mZstdDecompressor()\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dctx\u001b[38;5;241m.\u001b[39mstream_reader(compressed) \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m reader:\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m             record \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n",
      "\u001b[1;31mUnsupportedOperation\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import zstandard as zstd\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# === SETTINGS ===\n",
    "input_folder = r\"C:\\Users\\ninuy\\DSC 672\\Data\\reddit\\subreddits24\"      # <-- Change this to your input folder\n",
    "output_folder = r\"C:\\Users\\ninuy\\DSC 672\\Data\"  # <-- Change this to your output folder\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Full list of NBA teams\n",
    "nba_teams = [\n",
    "    \"Hawks\", \"Celtics\", \"Nets\", \"Hornets\", \"Bulls\",\n",
    "    \"Cavaliers\", \"Mavericks\", \"Nuggets\", \"Pistons\", \"Warriors\",\n",
    "    \"Rockets\", \"Pacers\", \"Clippers\", \"Lakers\", \"Grizzlies\",\n",
    "    \"Heat\", \"Bucks\", \"Timberwolves\", \"Pelicans\", \"Knicks\",\n",
    "    \"Thunder\", \"Magic\", \"76ers\", \"Suns\", \"Trail Blazers\",\n",
    "    \"Kings\", \"Spurs\", \"Raptors\", \"Jazz\", \"Wizards\"\n",
    "]\n",
    "\n",
    "def is_relevant(text):\n",
    "    return any(team.lower() in text.lower() for team in nba_teams)\n",
    "\n",
    "# Unix timestamp for Jan 1, 2017\n",
    "min_timestamp = 1483228800\n",
    "\n",
    "# === PROCESS EACH FILE ===\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".zst\"):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        print(f\"Processing: {filename}\")\n",
    "\n",
    "        results = []\n",
    "        with open(input_path, 'rb') as compressed:\n",
    "            dctx = zstd.ZstdDecompressor()\n",
    "            with dctx.stream_reader(compressed) as reader:\n",
    "                for line in reader:\n",
    "                    try:\n",
    "                        record = json.loads(line)\n",
    "\n",
    "                        # Skip if too old\n",
    "                        created_utc = record.get(\"created_utc\", 0)\n",
    "                        if created_utc < min_timestamp:\n",
    "                            continue\n",
    "\n",
    "                        # Only for submissions (title + selftext)\n",
    "                        title = record.get(\"title\", \"\")\n",
    "                        selftext = record.get(\"selftext\", \"\")\n",
    "                        text = f\"{title} {selftext}\".strip()\n",
    "\n",
    "                        # Skip removed or deleted content\n",
    "                        if not text or text in [\"[removed]\", \"[deleted]\"]:\n",
    "                            continue\n",
    "\n",
    "                        # Skip if not NBA-related\n",
    "                        if not is_relevant(text):\n",
    "                            continue\n",
    "\n",
    "                        sentiment = analyzer.polarity_scores(text)\n",
    "\n",
    "                        results.append({\n",
    "                            \"title\": title,\n",
    "                            \"selftext\": selftext,\n",
    "                            \"subreddit\": record.get(\"subreddit\", \"\"),\n",
    "                            \"created_utc\": created_utc,\n",
    "                            \"score\": record.get(\"score\", 0),\n",
    "                            \"compound\": sentiment[\"compound\"],\n",
    "                            \"positive\": sentiment[\"pos\"],\n",
    "                            \"neutral\": sentiment[\"neu\"],\n",
    "                            \"negative\": sentiment[\"neg\"]\n",
    "                        })\n",
    "\n",
    "                    except (json.JSONDecodeError, KeyError):\n",
    "                        continue\n",
    "\n",
    "        # Save results\n",
    "        if results:\n",
    "            df = pd.DataFrame(results)\n",
    "            base_filename = os.path.splitext(filename)[0]\n",
    "            output_path = os.path.join(output_folder, f\"{base_filename}_sentiment.csv\")\n",
    "            df.to_csv(output_path, index=False)\n",
    "            print(f\"Saved: {output_path}\")\n",
    "        else:\n",
    "            print(\"No relevant data found in:\", filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86c64eb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2021494379.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[23], line 18\u001b[1;36m\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import zstandard as zstd\n",
    "import json\n",
    "import io\n",
    "\n",
    "file_path = r\"C:\\Users\\ninuy\\DSC 672\\Data\\reddit\\subreddits24\\nba_submissions.zst\"  # Your file path\n",
    "with open(file_path, \"rb\") as f:\n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "    with dctx.stream_reader(f) as reader:\n",
    "        text_stream = io.TextIOWrapper(reader, encoding='utf-8')\n",
    "        for line in text_stream:\n",
    "            record = json.loads(line)\n",
    "            # Now you can process each record individually\n",
    "            print(record.get('selftext', ))  # Example: print first 100 chars of title\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3a6188c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\ninuy\\DSC 672\\Data\\reddit\\subreddits24\\nbacirclejerk_submissions.zst\n",
      "Processing file: C:\\Users\\ninuy\\DSC 672\\Data\\reddit\\subreddits24\\nbadiscussion_submissions.zst\n",
      "Processing file: C:\\Users\\ninuy\\DSC 672\\Data\\reddit\\subreddits24\\NBAForums_submissions.zst\n",
      "Processing file: C:\\Users\\ninuy\\DSC 672\\Data\\reddit\\subreddits24\\nba_submissions.zst\n",
      "Processing file: C:\\Users\\ninuy\\DSC 672\\Data\\reddit\\subreddits24\\sportsbetting_submissions.zst\n",
      "Sentiment data for NBA teams (post-2017) collected and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zstandard as zstd\n",
    "import json\n",
    "import io\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from datetime import datetime\n",
    "\n",
    "# Path to your folder containing .zst files\n",
    "folder_path = r\"C:\\Users\\ninuy\\DSC 672\\Data\\reddit\\subreddits24\"\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# NBA teams list (complete)\n",
    "nba_teams = [\n",
    "    \"Lakers\", \"Bucks\", \"Warriors\", \"Nets\", \"Heat\", \"76ers\", \"Celtics\", \"Nuggets\",\n",
    "    \"Suns\", \"Mavericks\", \"Clippers\", \"Raptors\", \"Knicks\", \"Pelicans\", \"Pacers\", \"Kings\",\n",
    "    \"Bulls\", \"Magic\", \"Spurs\", \"Hornets\", \"Wizards\", \"Hawks\", \"Grizzlies\", \"Rockets\",\n",
    "    \"Pistons\", \"Cavaliers\", \"Timberwolves\", \"Trail Blazers\", \"Jazz\", \"Thunder\",\n",
    "]\n",
    "\n",
    "# Minimum timestamp: Jan 1, 2017 UTC\n",
    "min_timestamp = int(datetime(2017, 1, 1).timestamp())\n",
    "\n",
    "# List to hold sentiment data\n",
    "all_team_data = []\n",
    "\n",
    "# Loop through all .zst files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".zst\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        \n",
    "        with open(file_path, \"rb\") as f:\n",
    "            dctx = zstd.ZstdDecompressor()\n",
    "            with dctx.stream_reader(f) as reader:\n",
    "                text_stream = io.TextIOWrapper(reader, encoding='utf-8')\n",
    "\n",
    "                for line in text_stream:\n",
    "                    try:\n",
    "                        submission = json.loads(line)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue  # Skip malformed lines\n",
    "\n",
    "                    created_utc = submission.get(\"created_utc\", 0)\n",
    "                    try:\n",
    "                        created_utc = int(created_utc)\n",
    "                    except (ValueError, TypeError):\n",
    "                        continue\n",
    "                    \n",
    "                    if created_utc < min_timestamp:\n",
    "                        continue\n",
    "\n",
    "                    title = submission.get(\"title\", \"\")\n",
    "                    selftext = submission.get(\"selftext\", \"\")\n",
    "\n",
    "                    combined_text = (title + \" \" + selftext).lower()\n",
    "\n",
    "                    matched_teams = [team for team in nba_teams if team.lower() in combined_text]\n",
    "                    if not matched_teams:\n",
    "                        continue\n",
    "\n",
    "                    sentiment_score = analyzer.polarity_scores(title + \" \" + selftext)\n",
    "\n",
    "                    for team in matched_teams:\n",
    "                        all_team_data.append({\n",
    "                            \"team\": team,\n",
    "                            \"subreddit\": submission.get(\"subreddit\", \"\"),\n",
    "                            \"post_title\": title,\n",
    "                            \"post_text\": selftext,\n",
    "                            \"positive\": sentiment_score[\"pos\"],\n",
    "                            \"neutral\": sentiment_score[\"neu\"],\n",
    "                            \"negative\": sentiment_score[\"neg\"],\n",
    "                            \"compound\": sentiment_score[\"compound\"],\n",
    "                            \"created_at\": created_utc,\n",
    "                            \"url\": submission.get(\"url\", \"\"),\n",
    "                        })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(all_team_data)\n",
    "df.to_csv(r\"C:\\Users\\ninuy\\DSC 672\\data\\sentiment\\nba_teams_sentiment_post2017.csv\", index=False)\n",
    "\n",
    "print(\"Sentiment data for NBA teams (post-2017) collected and saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
